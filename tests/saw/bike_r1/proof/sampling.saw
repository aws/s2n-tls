// sampling.saw Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0


///////////////////////////////////////////////////////////////////////////////
// Specifications

/* NOTES:
    `get_rand_mod_len` uses the results of the aes_ctr_prf in a way
    that guarantees termination only probabilistically (given a true
    random source).
*/
let get_rand_mod_len_spec = do {
  rand_pos_ptr <- out_ref i32;
  len <- crucible_fresh_var "len" i32;
  (prf_state, prf_state_ptr, ctx_ptr) <- ptr_to_fresh_aes_ctr_prf_state "prf_state";

  crucible_precond {{ len > 0 }};
  crucible_precond {{ prf_state.pos <= `AES256_BLOCK_SIZE }};

  crucible_execute_func
    [ rand_pos_ptr
    , (crucible_term {{ len }})
    , prf_state_ptr
    ];

  rand_pos' <- point_to i32 rand_pos_ptr "rand_pos'";
  prf_state' <- points_to_fresh_aes_ctr_prf_state prf_state_ptr ctx_ptr "prf_state'";

  crucible_postcond {{ rand_pos' < len }};
  crucible_postcond {{ prf_state'.pos <= `AES256_BLOCK_SIZE }};

  crucible_return (tm {{ fromInteger`{[32]} SUCCESS }});
};

let get_rand_mod_len_loop_spec = do {
  rand_pos_ptr <- out_ref i32;
  rand_pos_ptr_ptr <- crucible_alloc i64;
  crucible_points_to rand_pos_ptr_ptr rand_pos_ptr;
  (len, len_ptr) <- in_ref i32 "len";
  (prf_state, prf_state_ptr, ctx_ptr) <- ptr_to_fresh_aes_ctr_prf_state "prf_state";
  prf_state_ptr_ptr <- crucible_alloc i64;
  crucible_points_to prf_state_ptr_ptr prf_state_ptr;
  (mask, mask_ptr) <- in_ref i64 "mask";
  res_ptr <- crucible_alloc i32;

  crucible_precond {{ len > 0 }};
  crucible_precond {{ prf_state.pos <= `AES256_BLOCK_SIZE }};

  crucible_execute_func
    [ rand_pos_ptr_ptr
    , len_ptr
    , prf_state_ptr_ptr
    , mask_ptr
    , res_ptr
    ];

  rand_pos' <- point_to i32 rand_pos_ptr "rand_pos'";
  prf_state' <- points_to_fresh_aes_ctr_prf_state prf_state_ptr ctx_ptr "prf_state'";

  crucible_postcond {{ rand_pos' < len }};
  crucible_postcond {{ prf_state'.pos <= `AES256_BLOCK_SIZE }};

  crucible_return (tm {{ fromInteger`{[32]} SUCCESS }});
};

let sample_uniform_r_bits_spec = do {
  r_ptr <- out_ref r_t;
  (_, seed_ptr) <- in_ref seed_t "seed";
  must_be_odd <- crucible_fresh_var "must_be_odd" i32;
  crucible_execute_func [r_ptr, seed_ptr, tm must_be_odd];
  _ <- point_to r_t r_ptr "r'";
  crucible_return (tm {{ fromInteger`{[32]} SUCCESS }});
};

// NOTE: We parameterize by the lengths of the `a` and `wlist` parameters.
//       Must have weight < max (T1, FAKE_DV), or an assertion fails.
//       a_len_64 is the length of a `a` array, and is 1/8 times `a_len_bytes
let secure_set_bits_spec a_len_64 weight = do {
  (a ,a_ptr) <- inout_ref (i64_array_t a_len_64) "a";
  (wlist, wlist_ptr) <- in_ref (idx_array_t weight) "wlist";

  crucible_execute_func
    [ a_ptr
    , wlist_ptr
    , crucible_term {{ 8 * (`a_len_64:[32]) }}
    , crucible_term {{ `weight:[32] }}
    ];

  a' <- crucible_fresh_var "a'" (llvm_array a_len_64 i64);
  crucible_points_to a_ptr (crucible_term {{ a' }});

  return ();
};

let secure_set_bits_loop_spec a_len_64 weight = do {
  (a ,a_ptr) <- inout_ref (i64_array_t a_len_64) "a";
  a_ptr_ptr <- crucible_alloc i64;
  crucible_points_to a_ptr_ptr a_ptr;
  (wlist, wlist_ptr) <- in_ref (idx_array_t weight) "wlist";
  wlist_ptr_ptr <- crucible_alloc i64;
  crucible_points_to wlist_ptr_ptr wlist_ptr;
  a_len_8_ptr <- crucible_alloc i32;
  crucible_points_to a_len_8_ptr (crucible_term {{ 8 * (`a_len_64:[32]) }});
  weight_ptr <- crucible_alloc i32;
  crucible_points_to weight_ptr (crucible_term {{ `weight:[32] }});
  (qw_pos, qw_pos_ptr) <- in_ref (i64_array_t weight) "qw_pos";
  (bit_pos, bit_pos_ptr) <- in_ref (i64_array_t weight) "bit_pos";
  (tmp, tmp_ptr) <- ptr_to_fresh "tmp" i64;
  (qw, qw_ptr) <- ptr_to_fresh "qw" i32;
  (j, j_ptr) <- ptr_to_fresh "j" i32;
  mask_ptr <- crucible_alloc i64;

  crucible_precond {{ qw < `a_len_64 }};
  crucible_precond {{ j <= `weight }};

  crucible_execute_func
    [ a_ptr_ptr
    , wlist_ptr_ptr
    , a_len_8_ptr
    , weight_ptr
    , qw_pos_ptr
    , bit_pos_ptr
    , tmp_ptr
    , qw_ptr
    , j_ptr
    , mask_ptr
    ];

  a' <- crucible_fresh_var "a'" (llvm_array a_len_64 i64);
  crucible_points_to a_ptr (crucible_term {{ a' }});

  return ();
};

let generate_sparse_rep_spec weight len padded_len = do {
  a_ptr <- out_ref (i64_array_t (eval_int {{ `(padded_len / 8) : [64] }}));
  (wlist, wlist_ptr) <- inout_ref (idx_array_t weight) "wlist";
  (prf_state, prf_state_ptr, ctx_ptr) <- ptr_to_fresh_aes_ctr_prf_state "prf_state";

  // NOTE: Invariant
  crucible_precond {{ prf_state.pos <= `AES256_BLOCK_SIZE }};

  crucible_execute_func
    [ a_ptr
    , wlist_ptr
    , tm {{ `weight:[32] }}
    , tm {{ `len:[32] }}
    , tm {{ `padded_len:[32] }}
    , prf_state_ptr
    ];

  _a_res <- point_to (i64_array_t (eval_int {{ `(padded_len / 8) : [64] }})) a_ptr "a'";
  _wlist_res <- point_to (idx_array_t weight) wlist_ptr "wlist'";
  prf_state_res <- points_to_fresh_aes_ctr_prf_state prf_state_ptr ctx_ptr "prf_state'";

  // NOTE: Invariant
  crucible_postcond {{ prf_state_res.pos <= `AES256_BLOCK_SIZE }};

  crucible_return (tm {{ fromInteger`{[32]} SUCCESS }});
};

let generate_sparse_rep_loop_spec weight len padded_len = do {
  a_ptr <- out_ref (i64_array_t (eval_int {{ `(padded_len / 8) : [64] }}));
  a_ptr_ptr <- crucible_alloc i64;
  crucible_points_to a_ptr_ptr a_ptr;
  (wlist, wlist_ptr) <- inout_ref (idx_array_t weight) "wlist";
  wlist_ptr_ptr <- crucible_alloc i64;
  crucible_points_to wlist_ptr_ptr wlist_ptr;
  weight_ptr <- crucible_alloc i32;
  crucible_points_to weight_ptr (tm {{ `weight:[32] }});
  len_ptr <- crucible_alloc i32;
  crucible_points_to len_ptr (tm {{ `len:[32] }});
  padded_len_ptr <- crucible_alloc i32;
  crucible_points_to padded_len_ptr (tm {{ `padded_len:[32] }});
  (prf_state, prf_state_ptr, ctx_ptr) <- ptr_to_fresh_aes_ctr_prf_state "prf_state";
  prf_state_ptr_ptr <- crucible_alloc i64;
  crucible_points_to prf_state_ptr_ptr prf_state_ptr;
  res_ptr <- crucible_alloc i32;
  (ctr, ctr_ptr) <- ptr_to_fresh "ctr" i64;

  crucible_precond {{ prf_state.pos <= `AES256_BLOCK_SIZE }};
  crucible_precond {{ ctr < `weight }};

  crucible_execute_func
    [ a_ptr_ptr
    , wlist_ptr_ptr
    , weight_ptr
    , len_ptr
    , padded_len_ptr
    , prf_state_ptr_ptr
    , res_ptr
    , ctr_ptr
    ];

  _a_res <- point_to (i64_array_t (eval_int {{ `(padded_len / 8) : [64] }})) a_ptr "a'";
  _wlist_res <- point_to (idx_array_t weight) wlist_ptr "wlist'";
  prf_state_res <- points_to_fresh_aes_ctr_prf_state prf_state_ptr ctx_ptr "prf_state'";

  crucible_postcond {{ prf_state_res.pos <= `AES256_BLOCK_SIZE }};

  crucible_return (tm {{ fromInteger`{[32]} SUCCESS }});
};


///////////////////////////////////////////////////////////////////////////////
// Proof commands

get_rand_mod_len_loop_ov <- admit "__breakpoint__get_rand_mod_len_loop#get_rand_mod_len"
  [] get_rand_mod_len_loop_spec;
verify "__breakpoint__get_rand_mod_len_loop#get_rand_mod_len"
  [ get_rand_mod_len_loop_ov
  , aes_ctr_prf_4_ov
  ]
  get_rand_mod_len_loop_spec;
get_rand_mod_len_ov <- verify "get_rand_mod_len"
  [ get_rand_mod_len_loop_ov
  , bit_scan_reverse_ov
  ]
  get_rand_mod_len_spec;

sample_uniform_r_bits_ov <- verify "sample_uniform_r_bits"
  [ aes256_key_expansion_ov
  , aes256_enc_ov
  , aes256_free_ks_ov
  ]
  sample_uniform_r_bits_spec;

// NOTE: For generate_sparse_rep with T1 and N_BITS:
secure_set_bits_T1_N_loop_ov <- admit breakpoint__secure_set_bits_loop []
  (secure_set_bits_loop_spec N_PADDED_QW T1);
verify breakpoint__secure_set_bits_loop
  [ secure_set_bits_T1_N_loop_ov
  , secure_cmp32_ov
  ]
  (secure_set_bits_loop_spec N_PADDED_QW T1);
secure_set_bits_T1_N_ov <- verify secure_set_bits_fun_name
  [secure_set_bits_T1_N_loop_ov]
  (secure_set_bits_spec N_PADDED_QW T1);

// NOTE: For generate_sparse_rep with DV and R_BITS:
secure_set_bits_DV_R_loop_ov <- admit breakpoint__secure_set_bits_loop []
  (secure_set_bits_loop_spec R_PADDED_QW DV);
verify breakpoint__secure_set_bits_loop
  [ secure_set_bits_DV_R_loop_ov
  , secure_cmp32_ov
  ]
  (secure_set_bits_loop_spec R_PADDED_QW DV);
secure_set_bits_DV_R_ov <- verify secure_set_bits_fun_name
  [secure_set_bits_DV_R_loop_ov]
  (secure_set_bits_spec R_PADDED_QW DV);

enable_crucible_assert_then_assume;

generate_sparse_rep_T1_N_loop_ov <- admit breakpoint__generate_sparse_rep_loop
  [] (generate_sparse_rep_loop_spec T1 N_BITS N_PADDED_SIZE);
verify_pathsat breakpoint__generate_sparse_rep_loop
  [ generate_sparse_rep_T1_N_loop_ov
  , get_rand_mod_len_ov
  , secure_set_bits_T1_N_ov
  ]
  (generate_sparse_rep_loop_spec T1 N_BITS N_PADDED_SIZE);
generate_sparse_rep_T1_N_ov <- verify_pathsat generate_sparse_rep_fun_name
  [generate_sparse_rep_T1_N_loop_ov]
  (generate_sparse_rep_spec T1 N_BITS N_PADDED_SIZE);

generate_sparse_rep_DV_R_loop_ov <- admit breakpoint__generate_sparse_rep_loop
  [] (generate_sparse_rep_loop_spec DV R_BITS R_PADDED_SIZE);
verify_pathsat breakpoint__generate_sparse_rep_loop
  [ generate_sparse_rep_DV_R_loop_ov
  , get_rand_mod_len_ov
  , secure_set_bits_DV_R_ov
  ]
  (generate_sparse_rep_loop_spec DV R_BITS R_PADDED_SIZE);
generate_sparse_rep_DV_R_ov <- verify_pathsat generate_sparse_rep_fun_name
  [generate_sparse_rep_DV_R_loop_ov]
  (generate_sparse_rep_spec DV R_BITS R_PADDED_SIZE);

disable_crucible_assert_then_assume;

